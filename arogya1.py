# -*- coding: utf-8 -*-
"""arogya1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UHK126lmRNbGKNXshTeoAow6sOjmoDsj

INNITIAL SETUP
==============================================================================
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Change to dataset directory (update the path as per your Drive structure)
dataset_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray"
if os.path.exists(dataset_path):
    print("Dataset folder found! ðŸŽ‰")
    print("Files in dataset folder:", os.listdir(dataset_path))
else:
    print("âš  Dataset folder not found! Check the path.")

import torch
print("GPU Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU found")

import tensorflow as tf
import torch

# Check if TensorFlow detects the GPU
print("TensorFlow GPU:", "Available" if tf.config.list_physical_devices('GPU') else "Not Available")

# Check if PyTorch is using the GPU
print("PyTorch Device:", torch.device("cuda" if torch.cuda.is_available() else "cpu"))

"""PHASE 1
===============================================================================

"""

dataset_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/"
csv_path = os.path.join(dataset_path, "shenzhen_metadata.csv")

# Remove classes with only 1 sample
df_filtered = df[df['findings'].map(df['findings'].value_counts()) > 1]

# Now split the dataset
train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['findings'], random_state=42)

print(f"Training Samples: {len(train_df)}")
print(f"Testing Samples: {len(test_df)}")

from sklearn.model_selection import train_test_split

# Remove rare classes (with only 1 sample)
df_filtered = df[df['findings'].map(df['findings'].value_counts()) > 1]

# Perform stratified split
train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['findings'], random_state=42)

print(f"Training Samples: {len(train_df)}")
print(f"Testing Samples: {len(test_df)}")

# Dataset info
print("Dataset Info:")
print(df_filtered.info())

# Missing values check
print("\nMissing Values in Dataset:")
print(df_filtered.isnull().sum())

# Unique labels
print("\nUnique Labels:", df_filtered['findings'].unique())

df_filtered.loc[:, 'findings'] = df_filtered['findings'].replace({
    'PTB in the left lower field': 'PTB',
    'PTB in the left upper field': 'PTB',
    'secondary PTB  in the right upper field': 'PTB',
    'Bilateral secondary PTB': 'PTB',
    'PTB in the right lower field': 'PTB',
    'Old PTB in the right upper field': 'PTB',
    'PTB  in the right upper field': 'PTB',
    'left PTB': 'PTB',
    'PTB in the bilateral upper field': 'PTB',
    'left PTB,left pleurisy': 'PTB',
    'secondary PTB  in the bilateral upper field': 'PTB',
    'right upper PTB': 'PTB',
    'bilateral PTB, right pleurisy': 'PTB',
    'secondary PTB  in the bilateral upper fields': 'PTB',
    'left secondary PTB': 'PTB',
    'secondary PTB  in the left upper field': 'PTB',
    'right secondary PTB': 'PTB',
    'bilateral secondary PTB': 'PTB',
    'Left PTB': 'PTB',
    'PTB in the bilateral upper fields': 'PTB',
    'right lower field PTB': 'PTB'
})

print(df_filtered['findings'].value_counts())

df_filtered = df_filtered[df_filtered['findings'].map(df_filtered['findings'].value_counts()) > 1]

# Now, split the dataset
train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['findings'], random_state=42)

print(f"Training Samples: {len(train_df)}")
print(f"Testing Samples: {len(test_df)}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
sns.countplot(x=train_df['findings'])
plt.title("Class Distribution in Training Set")
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train_df['findings'] = le.fit_transform(train_df['findings'])
test_df['findings'] = le.transform(test_df['findings'])

print("Encoded Labels:", dict(enumerate(le.classes_)))

print(X_train.dtypes)

from sklearn.preprocessing import LabelEncoder

# Drop 'study_id' (since it's an image filename, not a feature)
X_train = X_train.drop(columns=['study_id'])
X_test = X_test.drop(columns=['study_id'])

# Encode 'sex' (convert Male/Female to 0/1)
le = LabelEncoder()
X_train['sex'] = le.fit_transform(X_train['sex'])
X_test['sex'] = le.transform(X_test['sex'])

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Baseline Model Accuracy: {accuracy:.2f}")

print(y_train.value_counts(normalize=True))

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train[['age']] = scaler.fit_transform(X_train[['age']])
X_test[['age']] = scaler.transform(X_test[['age']])

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Improved Model Accuracy: {accuracy:.2f}")

image_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images"

import os
from sklearn.model_selection import train_test_split

image_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images"

# Check if directory exists
if not os.path.exists(image_dir):
    raise ValueError(f"âš  The directory '{image_dir}' does not exist. Check if the path is correct.")

# Get list of image files
all_images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Ensure images are found
if not all_images:
    raise ValueError(f"âš  No image files found in '{image_dir}'. Check if the directory is correct.")

# Split images into train and test sets (80% train, 20% test)
train_images, test_images = train_test_split(all_images, test_size=0.2, random_state=42)

print(f"âœ… Train set: {len(train_images)} images")
print(f"âœ… Test set: {len(test_images)} images")

import shutil

train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Create directories if they donâ€™t exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Move images to train & test directories
for img_path in train_images:
    shutil.move(img_path, os.path.join(train_dir, os.path.basename(img_path)))

for img_path in test_images:
    shutil.move(img_path, os.path.join(test_dir, os.path.basename(img_path)))

print("âœ… Images moved successfully!")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define dataset paths
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Verify dataset paths
if not os.path.exists(train_dir):
    raise ValueError(f"âš  Training directory '{train_dir}' not found!")

if not os.path.exists(test_dir):
    raise ValueError(f"âš  Testing directory '{test_dir}' not found!")

# Data Augmentation & Normalization
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # Splitting training set for validation
)

test_datagen = ImageDataGenerator(rescale=1./255)

# Load Images as Batches
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),  # Adjust size based on model
    batch_size=32,
    class_mode='binary'  # Use 'categorical' if more than 2 classes
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

print("âœ… Data pipeline ready! ðŸš€")

import os
print(os.path.exists(csv_path))  # Should return True

import os
import pandas as pd

# Define the correct path to the CSV file
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"  # Update if needed

# Check if the file exists
print(os.path.exists(csv_path))  # Should return True

# Load the CSV file (only if it exists)
if os.path.exists(csv_path):
    df = pd.read_csv(csv_path)
    print(df.head())  # Display first few rows
else:
    print("File not found! Check the path.")

import pandas as pd

# Load the dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"  # Update if needed
df = pd.read_csv(csv_path)

# Check if it loaded correctly
print(df.head())

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/AROGYA/TB_Chest_Xray/

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"
df = pd.read_csv(file_path)

# Rename 'sex' to 'gender' for consistency
df.rename(columns={'sex': 'gender'}, inplace=True)

# Drop missing values in necessary columns
df = df.dropna(subset=['findings', 'age', 'gender'])

# Simplify 'findings' column: Group all tuberculosis cases under 'tuberculosis'
df['findings'] = df['findings'].apply(lambda x: 'tuberculosis' if 'TB' in x or 'PTB' in x or 'ptb' in x else x)

# âœ… 1ï¸âƒ£ Class Distribution: Normal vs. TB
plt.figure(figsize=(8, 5))
sns.countplot(x='findings', data=df, order=['normal', 'tuberculosis'], palette={'normal': 'blue', 'tuberculosis': 'red'})
plt.xlabel("Findings")
plt.ylabel("Count")
plt.title("Class Distribution: Normal vs. TB")
plt.show()

# âœ… 2ï¸âƒ£ Age Distribution of Patients
plt.figure(figsize=(8, 5))
sns.histplot(df['age'], bins=30, kde=True, color="purple")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.title("Age Distribution of Patients")
plt.show()

# âœ… 3ï¸âƒ£ TB Cases by Gender
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="gender", hue="findings", palette="tab10", order=['Male', 'Female'])
plt.xlabel("Gender")
plt.ylabel("Count")
plt.title("TB Cases by Gender")
plt.legend(title="Findings", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

import os
print("Metadata File Exists:", os.path.exists("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"))

import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"
df = pd.read_csv(file_path)

# Standard Processing
df.columns = df.columns.str.lower()
if 'sex' in df.columns:
    df.rename(columns={'sex': 'gender'}, inplace=True)
df.dropna(subset=['findings', 'age', 'gender'], inplace=True)
df['findings'] = df['findings'].apply(lambda x: 'tuberculosis' if 'tb' in x.lower() else 'normal')

# âœ… Save the Processed File
processed_path = "/content/shenzhen_processed.csv"
df.to_csv(processed_path, index=False)

print(f"âœ… Processed file saved at: {processed_path}")

import shutil
import os

# Define source and destination paths
source_path = "/content/shenzhen_processed.csv"
destination_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"

# Check if source file exists
if os.path.exists(source_path):
    # Move the file
    shutil.move(source_path, destination_path)
    print(f"âœ… File moved successfully to: {destination_path}")
else:
    print("âŒ Source file does not exist!")

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"  # Ensure correct path

# Check if file exists before loading
if not os.path.exists(file_path):
    raise FileNotFoundError(f"âŒ Error: File not found at {file_path}")

print("âœ… Metadata file found. Loading dataset...")

df = pd.read_csv(file_path)

# Convert column names to lowercase for consistency
df.columns = df.columns.str.lower()

# Rename 'sex' column to 'gender' for compatibility with Arogya
if 'sex' in df.columns:
    df.rename(columns={'sex': 'gender'}, inplace=True)

# Check for missing values and drop them if found
df.dropna(subset=['findings', 'age', 'gender'], inplace=True)

# Standardize 'findings' values (Group all TB-related labels under 'tuberculosis')
df['findings'] = df['findings'].apply(lambda x: 'tuberculosis' if 'tb' in x.lower() else 'normal')

print("âœ… Data preprocessing complete.")

# Create the output directory if it doesn't exist
output_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray"
os.makedirs(output_dir, exist_ok=True)

# Visualizing Class Distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='findings', data=df, order=['normal', 'tuberculosis'], hue='findings',
              palette={'normal': 'blue', 'tuberculosis': 'red'}, legend=False)
plt.xlabel("Findings")
plt.ylabel("Count")
plt.title("Class Distribution: Normal vs. TB")
plt.show()

# Visualizing Age Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['age'], bins=30, kde=True, color="purple")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.title("Age Distribution of Patients")
plt.show()

# Visualizing TB Cases by Gender
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="gender", hue="findings", palette="tab10")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.title("TB Cases by Gender")
plt.legend(title="Findings", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

# Save processed data for Arogya integration
output_file = os.path.join(output_dir, "shenzhen_processed.csv")
df.to_csv(output_file, index=False)

print(f"âœ… Processed file saved at: {output_file}")

import os
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
print("File Exists:", os.path.exists(file_path))

import os

all_files = []
for root, dirs, files in os.walk("/content/drive/MyDrive/AROGYA/"):
    all_files.extend(files)

print(all_files)

import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(file_path)

# Display basic info
df.info()
df.head()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['age']] = scaler.fit_transform(df[['age']])

print(df.head())

processed_file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
df.to_csv(processed_file_path, index=False)

print("Processed dataset saved successfully!")

df.rename(columns=lambda x: x.strip(), inplace=True)  # Removes unwanted spaces

import pandas as pd

# Load Data
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"

try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print("Error: File not found. Ensure the file exists in the directory.")
    df = pd.DataFrame()  # Create an empty DataFrame as a fallback

# Ensure DataFrame is not empty
if df.empty:
    print("Warning: DataFrame is empty. Check data source.")

# Rename 'gender' to 'sex' (if it exists)
if 'gender' in df.columns:
    df.rename(columns={'gender': 'sex'}, inplace=True)

# Check unique values in 'sex' before processing
if 'sex' in df.columns:
    print("Unique values in 'sex' before processing:", df['sex'].unique())

    # Handle missing values in 'sex'
    if not df['sex'].mode().empty:
        mode_sex = df['sex'].mode()[0]  # Get the mode
        df['sex'] = df['sex'].fillna(mode_sex)
    else:
        df['sex'] = df['sex'].fillna("Unknown")

    # âœ… Fix Mapping Issue: Convert 'Male' â†’ 0, 'Female' â†’ 1
    df['sex'] = df['sex'].map({'Male': 0, 'Female': 1})

# Handle missing values in 'age' (if column exists)
if 'age' in df.columns:
    df['age'] = df['age'].fillna(df['age'].median())

# Handle missing values in 'findings' (if column exists)
if 'findings' in df.columns:
    if not df['findings'].mode().empty:
        df['findings'] = df['findings'].fillna(df['findings'].mode()[0])
    else:
        df['findings'] = df['findings'].fillna("Unknown")

# Display first few rows to verify changes
print(df.head())

# Check for missing values after processing
print("Missing values after processing:")
print(df.isnull().sum())

# Print final column names
print("Final column names:", df.columns)

# Save processed file
final_file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
df.to_csv(final_file_path, index=False)

print("âœ… Processing complete! File saved as 'shenzhen_final_processed.csv'.")

import os
import cv2
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Paths
csv_file = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/"
processed_image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/processed_images/"

# Load dataset
df = pd.read_csv(csv_file)

# Ensure output folder exists
if not os.path.exists(processed_image_folder):
    os.makedirs(processed_image_folder)

# Image Processing Configuration
IMG_SIZE = (224, 224)  # ResNet50 standard input size

# Check for missing images
missing_images = []
for study_id in df["study_id"]:
    image_path = os.path.join(image_folder, study_id)
    if not os.path.exists(image_path):
        missing_images.append(study_id)
        continue

    # Read and preprocess image
    img = cv2.imread(image_path)
    img = cv2.resize(img, IMG_SIZE)  # Resize to 224x224
    img = img / 255.0  # Normalize pixel values

    # Save processed image
    processed_path = os.path.join(processed_image_folder, study_id)
    cv2.imwrite(processed_path, (img * 255).astype(np.uint8))  # Convert back to 0-255 for saving

# Print missing images
if missing_images:
    print(f"âš ï¸ Missing {len(missing_images)} images:")
    print(missing_images)
else:
    print("âœ… All images are present and processed.")

# Data Augmentation (If Needed)
augment = False  # Change to True if you want augmentation

if augment:
    datagen = ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True
    )

    for study_id in df["study_id"]:
        img_path = os.path.join(processed_image_folder, study_id)
        img = cv2.imread(img_path)
        img = np.expand_dims(img, axis=0)  # Convert to batch format

        # Generate augmented images
        aug_iter = datagen.flow(img, batch_size=1)
        aug_img = next(aug_iter)[0].astype(np.uint8)

        # Save augmented image
        aug_path = os.path.join(processed_image_folder, f"aug_{study_id}")
        cv2.imwrite(aug_path, aug_img)

    print("âœ… Data augmentation completed.")

import os
import shutil

# Define paths
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Ensure class folders exist
for category in ["Normal", "TB"]:
    os.makedirs(os.path.join(train_dir, category), exist_ok=True)
    os.makedirs(os.path.join(test_dir, category), exist_ok=True)

# Function to move images based on their labels
def move_images(source_dir):
    for filename in os.listdir(source_dir):
        src_path = os.path.join(source_dir, filename)

        if os.path.isdir(src_path):  # Skip directories
            continue

        # Determine the category based on filename
        if filename.endswith("_0.png"):
            dest_path = os.path.join(source_dir, "Normal", filename)
        elif filename.endswith("_1.png"):
            dest_path = os.path.join(source_dir, "TB", filename)
        else:
            print(f"âš ï¸ Unknown category: {filename} (Not moved)")
            continue

        # Move the file
        shutil.move(src_path, dest_path)

# Apply the function to both train and test directories
move_images(train_dir)
move_images(test_dir)

print("âœ… Images have been successfully moved to their categories!")

import os
import shutil

# Define paths
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Create class subfolders if they donâ€™t exist
for category in ["Normal", "TB"]:
    os.makedirs(os.path.join(train_dir, category), exist_ok=True)
    os.makedirs(os.path.join(test_dir, category), exist_ok=True)

# Function to move images based on filename
def organize_images(directory):
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)

        # Skip if it's already in a folder
        if os.path.isdir(file_path):
            continue

        # Move images based on category
        if "normal" in filename.lower():
            shutil.move(file_path, os.path.join(directory, "Normal", filename))
        elif "tb" in filename.lower() or "tuberculosis" in filename.lower():
            shutil.move(file_path, os.path.join(directory, "TB", filename))
        else:
            print(f"âš ï¸ Unknown category: {filename} (Not moved)")

# Move images in train and test sets
print("ðŸ“‚ Organizing train images...")
organize_images(train_dir)

print("ðŸ“‚ Organizing test images...")
organize_images(test_dir)

print("âœ… Images moved successfully!")

import os

train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

for category in ["Normal", "TB"]:
    train_path = os.path.join(train_dir, category)
    test_path = os.path.join(test_dir, category)

    print(f"ðŸ“‚ Checking: {train_path}")
    print(f"ðŸ–¼ï¸ Train Images: {len(os.listdir(train_path)) if os.path.exists(train_path) else 'Folder Not Found'}")

    print(f"ðŸ“‚ Checking: {test_path}")
    print(f"ðŸ–¼ï¸ Test Images: {len(os.listdir(test_path)) if os.path.exists(test_path) else 'Folder Not Found'}\n")

import pandas as pd
import os
import cv2
import numpy as np
import tensorflow as tf

# Load dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Step 1: Check if 'gender' column exists before renaming
if "gender" in df.columns:
    df.rename(columns={"gender": "sex"}, inplace=True)

# Step 2: Handling Missing Values
df.loc[:, 'sex'] = df.get('sex', pd.Series(['Unknown'] * len(df)))  # Fill missing column
df.loc[:, 'age'] = df.get('age', pd.Series([df['age'].median()] * len(df)))  # Fill missing ages

# Convert 'sex' column to numerical values
df['sex'] = df['sex'].map({'Male': 0, 'Female': 1}).fillna(-1)  # Assign -1 for Unknown

# Step 3: Image Processing
image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/"
processed_images = []

for study_id in df['study_id']:
    # Ensure study_id doesn't have extra .png
    study_id = str(study_id)  # Convert to string in case of NaN
    if not study_id.endswith(".png"):
        study_id += ".png"

    img_path = os.path.join(image_folder, study_id)

    if os.path.exists(img_path):  # Ensure file exists
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)  # Load in color (for ResNet)
        img = cv2.resize(img, (224, 224))  # Resize for ResNet50

        # Apply proper ResNet preprocessing
        img = tf.keras.applications.resnet50.preprocess_input(img)

        processed_images.append(img)
    else:
        print(f"âš ï¸ Image not found: {img_path}")

# Convert list to NumPy array and save
if processed_images:
    processed_images = np.array(processed_images)
    np.save("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen/processed_images.npy", processed_images)
    print(f"âœ… Processed {len(processed_images)} images successfully!")
else:
    print("âŒ No valid images processed. Check your dataset.")

import os

# Check if the directory exists
if os.path.exists(image_folder):
    print("âœ… Image folder exists.")
    print("ðŸ“‚ Sample files:", os.listdir(image_folder)[:10])  # List first 10 files
else:
    print("âŒ Image folder not found! Check the path.")

import pandas as pd

# Define CSV path
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"

# Load dataset
df = pd.read_csv(csv_path)  # âœ… Now df is defined

import pandas as pd

# Load dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Print column names to check
print("ðŸ” Column names in dataset:", df.columns.tolist())  # Debugging step

df.rename(columns={"gender": "sex"}, inplace=True)

if df.empty:
    raise ValueError("âŒ CSV file is empty or not loading correctly!")

print(df.head())  # Show first few rows to verify data

# Create a new copy to avoid chained assignment warning
df = df.copy()

# Fill missing values
df['sex'] = df['sex'].fillna('Unknown')
df['age'] = df['age'].fillna(df['age'].median())

import os
import pandas as pd

# Load dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Check if 'study_id' exists in DataFrame
if 'study_id' not in df.columns:
    raise ValueError("âŒ Column 'study_id' not found in dataset. Check your CSV file!")

# Define the image folder path
image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/"

# Check image paths
for study_id in df['study_id']:
    # Ensure we don't add ".png" again if it's already there
    if not study_id.endswith(".png"):
        study_id += ".png"

    img_path = os.path.join(image_folder, study_id)

    if not os.path.exists(img_path):
        print(f"âš ï¸ Image not found: {img_path}")

"""Train the Model (Use GPU for Speed âš¡)


âœ… Start with ResNet50 (Pretrained on ImageNet).

âœ… Use Google Colab Pro or Kaggle GPUs for faster training.


âœ… Save model weights after each epoch to avoid retraining from scratch.

"""

y = df['findings'].values  # Use 'findings' as the label

print(df['findings'].unique())  # Check what labels are present

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y = encoder.fit_transform(df['findings'])  # 'normal' â†’ 0, 'tuberculosis' â†’ 1

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")

"""TRAINING
=========================================================================

"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Load Pretrained ResNet50 (without top layers)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers
x = Flatten()(base_model.output)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)  # Binary classification (Normal vs TB)

# Compile model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.summary()  # Check model architecture

history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))

from tensorflow.keras.applications import VGG16

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# à¤…à¤¬ à¤†à¤–à¤¿à¤°à¥€ à¤•à¥‡ 20 à¤²à¥‡à¤¯à¤°à¥à¤¸ à¤•à¥‹ trainable à¤¬à¤¨à¤¾à¤¨à¤¾
for layer in base_model.layers[-20:]:
    layer.trainable = True

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    rescale=1./255
)

from tensorflow.keras.layers import Flatten, Dense, Dropout

x = Flatten()(base_model.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(1, activation='sigmoid')(x)

from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

"""doneeeeeeeeee"""

import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"  # Ensure this is correct
df = pd.read_csv(file_path)

# Check if the dataset is loaded properly
print(df.head())  # Display first few rows
print(df.columns)  # Display column names

import os
print(os.listdir("/content/drive/MyDrive/AROGYA/TB_Chest_Xray"))

df = pd.read_csv(file_path, encoding='latin1')

import pandas as pd

# Correct file path
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"

# Load dataset with encoding handling
try:
    df = pd.read_csv(file_path, encoding='utf-8')  # Try utf-8 encoding first
    print(df.head())  # Display first few rows
    print(df.columns)  # Display column names
except UnicodeDecodeError:
    df = pd.read_csv(file_path, encoding='ISO-8859-1')  # Fallback encoding
    print(df.head())
    print(df.columns)

print(df["findings"].value_counts())

X_train = df[['sex', 'age']].values  # Features
y_train = df['findings'].values  # Labels

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)  # Convert labels to numbers

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(class_weights))
print(class_weights)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
df = pd.read_csv(file_path, encoding='utf-8')

# Display dataset summary
print(df.head())
print(df["findings"].value_counts())

# Feature & label extraction
X = df[['sex', 'age']].values
y = df['findings'].values

# Label encoding
encoder = LabelEncoder()
y = encoder.fit_transform(y)  # Convert labels to numbers (0=Normal, 1=TB)

# ðŸ”¹ Normalize Age (Feature Scaling)
scaler = MinMaxScaler()
X[:, 1] = scaler.fit_transform(X[:, 1].reshape(-1, 1)).flatten()

# Split dataset into Train & Validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Compute class weights (for imbalanced dataset)
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(class_weights))
print(class_weights)

# âœ… Improved Model Architecture
model = Sequential([
    Input(shape=(X_train.shape[1],)),  # Fix UserWarning
    Dense(32, activation='relu'),
    Dropout(0.2),  # Regularization
    Dense(16, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # Binary classification
])

# âœ… Compile with lower learning rate
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# âœ… Train the Model
history = model.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=20,  # More epochs
    batch_size=32,
    class_weight=class_weights
)

"""NOW WORKIG TO IMPROVE THE EFFICIENCY AND FURTHER

"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rescale=1./255,      # Normalize pixels
    rotation_range=10,   # Rotate images slightly
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True
)

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model

base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
x = Flatten()(base_model.output)
x = Dense(256, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1, activation="sigmoid")(x)  # Binary classification

model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])



import os
print(os.listdir("/content"))

from google.colab import drive
drive.mount('/content/drive')

import os
print(os.listdir("/content/drive/MyDrive/AROGYA/TB_Chest_Xray"))

train_data = datagen.flow_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train",  # Use the correct subfolder
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary"
)

val_data = datagen.flow_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test",  # Use the correct subfolder
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary"
)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=50,  # Increase if needed
    callbacks=[lr_scheduler]  # Ensure lr_scheduler is defined
)

"""TESTING AND SAVINGS THE THINGS DONE
==============================================================================

"""

import os

h5_model_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.h5"
print("File exists:", os.path.exists(h5_model_path))

import numpy as np
import tensorflow as tf

# Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Create a random test input (assuming input shape is 224x224x3)
test_input = np.random.rand(1, 224, 224, 3).astype(np.float32)

# Run inference
interpreter.set_tensor(input_details[0]['index'], test_input)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

print("ðŸŸ¢ TFLite Model Output:", output_data)

import cv2

# Load and preprocess an actual X-ray image
image_path = "/content/drive/MyDrive/AROGYA/sample_xray.jpg"
image = cv2.imread(image_path)
image = cv2.resize(image, (224, 224))  # Resize to match model input size
image = image.astype(np.float32) / 255.0  # Normalize
image = np.expand_dims(image, axis=0)  # Add batch dimension

# Run inference
interpreter.set_tensor(input_details[0]['index'], image)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

print("ðŸŸ¢ TFLite Model Output:", output_data)

import tensorflow as tf

# Define paths
h5_model_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.h5"
tflite_model_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite"

# Load the existing .h5 model
model = tf.keras.models.load_model(h5_model_path)

# Convert to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the .tflite model
with open(tflite_model_path, "wb") as f:
    f.write(tflite_model)

print("âœ… Model successfully converted and saved at:", tflite_model_path)

"""
TO MOUNT DRIVE EXECUTE THE FOLLOWING CODES
============================================================================="""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

import pandas as pd

csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Print the first few rows
print(df.head())

# Print all column names
print(df.columns)

df.columns = df.columns.str.strip()  # Removes leading/trailing spaces

import pandas as pd

csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Print all column names
print("Columns in CSV:", df.columns.tolist())

print(df.head())  # Shows first few rows

import os

# Define the directory where images are stored
image_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/"

# Generate image paths (assuming filenames match study_id)
df["image_path"] = df["study_id"].astype(str) + ".png"  # Adjust extension if needed
df["image_path"] = df["image_path"].apply(lambda x: os.path.join(image_dir, x))

# Save the updated CSV with image paths
df.to_csv("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv", index=False)

print("Image paths added successfully! âœ…")

image_paths = df['image_path'].values
labels = df['findings'].apply(lambda x: 1 if x.lower() == 'tuberculosis' else 0).values

import os

# Fix duplicate extensions
image_paths = [os.path.join("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/", path.replace(".png.png", ".png")) for path in df['study_id']]

# Verify the first corrected path
print(image_paths[0])
print(os.path.exists(image_paths[0]))  # Should print True now

from PIL import Image
img = Image.open(image_paths[0])
img.show()  # Display the image

from PIL import Image

# Open and display the image
img = Image.open(image_paths[0])
img.show()  # Should display the image

import os
import cv2
import matplotlib.pyplot as plt

image_path = image_paths[0]  # First image path

if os.path.exists(image_path):
    print(f"File exists: {image_path}")
    img = cv2.imread(image_path, cv2.IMREAD_COLOR)

    if img is not None:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
        plt.imshow(img)
        plt.title("X-ray Image")
        plt.show()
    else:
        print("âš  OpenCV failed to load the image. Check file format or permissions.")
else:
    print("ðŸš¨ Image file does not exist. Verify the file path!")

import cv2
import matplotlib.pyplot as plt

# Load the image
img = cv2.imread(image_path)

# Check if the image was loaded successfully
if img is None:
    print("ðŸš¨ Error: Failed to load the image. Check the file format and path!")
else:
    # Convert BGR to RGB (since OpenCV loads images in BGR format)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Display the image
    plt.imshow(img)
    plt.axis("off")  # Hide axes
    plt.title("Chest X-ray")
    plt.show()

"""Backend testing
==============================================================================
"""

import tensorflow as tf
print(tf.__version__)  # Check TensorFlow version

import tensorflow as tf

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")

# Allocate tensors
interpreter.allocate_tensors()

# Get input details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Print input & output tensor details
print("Input details:", input_details)
print("Output details:", output_details)

import cv2
import numpy as np
import tensorflow as tf

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")
interpreter.allocate_tensors()

# Get input & output tensor indices
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Load and preprocess an actual X-ray image
image_path = "/content/drive/MyDrive/AROGYA/sample_xray.jpg"
image = cv2.imread(image_path)

# Verify if the image was loaded correctly
if image is None:
    raise FileNotFoundError(f"âŒ Image not found at: {image_path}")

# Resize & normalize image
image = cv2.resize(image, (224, 224))
image = image.astype(np.float32) / 255.0  # Normalize to [0,1]
image = np.expand_dims(image, axis=0)  # Add batch dimension

# Set input tensor
interpreter.set_tensor(input_details[0]['index'], image)

# Run inference
interpreter.invoke()

# Get output tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

# Print the result
print("ðŸŸ¢ Model Output:", output_data)

# Interpret output (assuming binary classification: [0] = Normal, [1] = TB)
prob_TB = output_data[0][1]  # Probability of TB
prob_Normal = output_data[0][0]  # Probability of Normal

if prob_TB > prob_Normal:
    print(f"ðŸ”´ Prediction: Tuberculosis detected with {prob_TB:.2%} confidence")
else:
    print(f"ðŸŸ¢ Prediction: Normal with {prob_Normal:.2%} confidence")

import cv2
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from google.colab import files
from PIL import Image

# 1ï¸âƒ£ Ask user to upload an X-ray image
uploaded = files.upload()  # Opens file upload prompt

# Get the uploaded file name
image_name = list(uploaded.keys())[0]
print(f"âœ… Uploaded file: {image_name}")

# Load the uploaded image
image = Image.open(image_name).convert("L")  # Convert to grayscale

# Display the uploaded image
plt.imshow(image, cmap='gray')
plt.title("Uploaded X-ray Image")
plt.axis("off")
plt.show()

# Convert image to OpenCV format
image = np.array(image)

# 2ï¸âƒ£ Preprocess the image for the model
image = cv2.resize(image, (224, 224))  # Resize to model input size
image = np.expand_dims(image, axis=-1)  # Add channel dimension (grayscale)
image = np.repeat(image, 3, axis=-1)  # Convert grayscale to 3-channel RGB
image = image.astype(np.float32) / 255.0  # Normalize to [0,1]
image = np.expand_dims(image, axis=0)  # Add batch dimension

# 3ï¸âƒ£ Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")
interpreter.allocate_tensors()

# Get input & output tensor details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Set the input tensor
interpreter.set_tensor(input_details[0]['index'], image)

# Run inference
interpreter.invoke()

# Get the output tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

# Print the result
print("ðŸŸ¢ Model Output:", output_data)

# 4ï¸âƒ£ Interpret output (assuming binary classification: [0] = Normal, [1] = TB)
prob_TB = output_data[0][1]  # Probability of TB
prob_Normal = output_data[0][0]  # Probability of Normal

if prob_TB > prob_Normal:
    print(f"ðŸ”´ Prediction: Tuberculosis detected with {prob_TB:.2%} confidence")
else:
    print(f"ðŸŸ¢ Prediction: Normal with {prob_Normal:.2%} confidence")

pip install --upgrade tensorflow

"""


Flask API DEPENDENCIES SET UP TRY
====================================
"""

from google.colab import drive
drive.mount('/content/drive')

import os
print(os.listdir("/content/drive/MyDrive/AROGYA/TB_Chest_Xray"))

import tensorflow as tf

train_data = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train",  # Updated path
    image_size=(224, 224),
    batch_size=32
)

val_data = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test",  # Updated path
    image_size=(224, 224),
    batch_size=32
)

import matplotlib.pyplot as plt
import numpy as np

class_names = train_data.class_names  # Get class labels

plt.figure(figsize=(10, 10))
for images, labels in train_data.take(1):  # Take one batch
    for i in range(9):  # Show 9 images
        plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
plt.show()

print("Train dataset shape:", train_data)
print("Validation dataset shape:", val_data)
print("Class labels:", train_data.class_names)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define CNN model
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),  # Reduce overfitting
    layers.Dense(2, activation='softmax')  # 2 classes: Normal, TB
])

# Compile model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Summary of the model
model.summary()

# Load dataset using ImageDataGenerator
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define directories
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
val_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Data Augmentation & Preprocessing
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# Load the training and validation datasets
train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse'  # Matches sparse_categorical_crossentropy
)

val_data = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse'
)

# Now train the model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    verbose=1
)

"""FRONTEND
====================

"""

from google.colab import drive
drive.mount('/content/drive')

!pip install flask-ngrok

!pip install flask

!pip install flask flask-ngrok pyngrok numpy opencv-python tensorflow

import ipywidgets as widgets
from IPython.display import display
import requests

# Create a file upload widget
upload = widgets.FileUpload(accept='.jpg,.png,.jpeg', multiple=False)
display(upload)

def upload_and_predict(change):
    if upload.value:
        file_info = next(iter(upload.value.values()))
        file_name = file_info['metadata']['name']

        # Save uploaded file
        with open(file_name, 'wb') as f:
            f.write(file_info['content'])

        # API endpoint
        url = "http://xyz.ngrok.io/predict"

        # Upload the chosen image
        try:
            with open(file_name, 'rb') as f:
                files = {'file': f}
                response = requests.post(url, files=files)

            # Check if the response is valid JSON
            try:
                result = response.json()
                print("Prediction Result:", result)
            except requests.exceptions.JSONDecodeError:
                print("Error: Response is not valid JSON. Response text:", response.text)

        except requests.exceptions.RequestException as e:
            print("Request failed:", e)

upload.observe(upload_and_predict, names='value')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define ImageDataGenerator for loading dataset
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_data = train_datagen.flow_from_directory(
    'dataset_directory',  # Replace with actual dataset path
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='training'
)

val_data = train_datagen.flow_from_directory(
    'dataset_directory',
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='validation'
)

# Define the model
model = keras.Sequential([
    layers.Input(shape=(224, 224, 3)),  # Corrected input shape
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(2, activation='softmax')  # 2 classes (Normal, TB)
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    verbose=1
)