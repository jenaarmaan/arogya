# -*- coding: utf-8 -*-
"""arogya1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UHK126lmRNbGKNXshTeoAow6sOjmoDsj

INNITIAL SETUP
==============================================================================
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Change to dataset directory (update the path as per your Drive structure)
dataset_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray"
if os.path.exists(dataset_path):
    print("Dataset folder found! üéâ")
    print("Files in dataset folder:", os.listdir(dataset_path))
else:
    print("‚ö† Dataset folder not found! Check the path.")

import torch
print("GPU Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU found")

import tensorflow as tf
import torch

# Check if TensorFlow detects the GPU
print("TensorFlow GPU:", "Available" if tf.config.list_physical_devices('GPU') else "Not Available")

# Check if PyTorch is using the GPU
print("PyTorch Device:", torch.device("cuda" if torch.cuda.is_available() else "cpu"))

"""PHASE 1
===============================================================================

"""

dataset_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/"
csv_path = os.path.join(dataset_path, "shenzhen_metadata.csv")

# Remove classes with only 1 sample
df_filtered = df[df['findings'].map(df['findings'].value_counts()) > 1]

# Now split the dataset
train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['findings'], random_state=42)

print(f"Training Samples: {len(train_df)}")
print(f"Testing Samples: {len(test_df)}")

from sklearn.model_selection import train_test_split

# Remove rare classes (with only 1 sample)
df_filtered = df[df['findings'].map(df['findings'].value_counts()) > 1]

# Perform stratified split
train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['findings'], random_state=42)

print(f"Training Samples: {len(train_df)}")
print(f"Testing Samples: {len(test_df)}")

# Dataset info
print("Dataset Info:")
print(df_filtered.info())

# Missing values check
print("\nMissing Values in Dataset:")
print(df_filtered.isnull().sum())

# Unique labels
print("\nUnique Labels:", df_filtered['findings'].unique())

df_filtered.loc[:, 'findings'] = df_filtered['findings'].replace({
    'PTB in the left lower field': 'PTB',
    'PTB in the left upper field': 'PTB',
    'secondary PTB  in the right upper field': 'PTB',
    'Bilateral secondary PTB': 'PTB',
    'PTB in the right lower field': 'PTB',
    'Old PTB in the right upper field': 'PTB',
    'PTB  in the right upper field': 'PTB',
    'left PTB': 'PTB',
    'PTB in the bilateral upper field': 'PTB',
    'left PTB,left pleurisy': 'PTB',
    'secondary PTB  in the bilateral upper field': 'PTB',
    'right upper PTB': 'PTB',
    'bilateral PTB, right pleurisy': 'PTB',
    'secondary PTB  in the bilateral upper fields': 'PTB',
    'left secondary PTB': 'PTB',
    'secondary PTB  in the left upper field': 'PTB',
    'right secondary PTB': 'PTB',
    'bilateral secondary PTB': 'PTB',
    'Left PTB': 'PTB',
    'PTB in the bilateral upper fields': 'PTB',
    'right lower field PTB': 'PTB'
})

print(df_filtered['findings'].value_counts())

df_filtered = df_filtered[df_filtered['findings'].map(df_filtered['findings'].value_counts()) > 1]

# Now, split the dataset
train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['findings'], random_state=42)

print(f"Training Samples: {len(train_df)}")
print(f"Testing Samples: {len(test_df)}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
sns.countplot(x=train_df['findings'])
plt.title("Class Distribution in Training Set")
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
train_df['findings'] = le.fit_transform(train_df['findings'])
test_df['findings'] = le.transform(test_df['findings'])

print("Encoded Labels:", dict(enumerate(le.classes_)))

print(X_train.dtypes)

from sklearn.preprocessing import LabelEncoder

# Drop 'study_id' (since it's an image filename, not a feature)
X_train = X_train.drop(columns=['study_id'])
X_test = X_test.drop(columns=['study_id'])

# Encode 'sex' (convert Male/Female to 0/1)
le = LabelEncoder()
X_train['sex'] = le.fit_transform(X_train['sex'])
X_test['sex'] = le.transform(X_test['sex'])

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Baseline Model Accuracy: {accuracy:.2f}")

print(y_train.value_counts(normalize=True))

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train[['age']] = scaler.fit_transform(X_train[['age']])
X_test[['age']] = scaler.transform(X_test[['age']])

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Improved Model Accuracy: {accuracy:.2f}")

image_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images"

import os
from sklearn.model_selection import train_test_split

image_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images"

# Check if directory exists
if not os.path.exists(image_dir):
    raise ValueError(f"‚ö† The directory '{image_dir}' does not exist. Check if the path is correct.")

# Get list of image files
all_images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

# Ensure images are found
if not all_images:
    raise ValueError(f"‚ö† No image files found in '{image_dir}'. Check if the directory is correct.")

# Split images into train and test sets (80% train, 20% test)
train_images, test_images = train_test_split(all_images, test_size=0.2, random_state=42)

print(f"‚úÖ Train set: {len(train_images)} images")
print(f"‚úÖ Test set: {len(test_images)} images")

import shutil

train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Create directories if they don‚Äôt exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Move images to train & test directories
for img_path in train_images:
    shutil.move(img_path, os.path.join(train_dir, os.path.basename(img_path)))

for img_path in test_images:
    shutil.move(img_path, os.path.join(test_dir, os.path.basename(img_path)))

print("‚úÖ Images moved successfully!")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define dataset paths
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Verify dataset paths
if not os.path.exists(train_dir):
    raise ValueError(f"‚ö† Training directory '{train_dir}' not found!")

if not os.path.exists(test_dir):
    raise ValueError(f"‚ö† Testing directory '{test_dir}' not found!")

# Data Augmentation & Normalization
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # Splitting training set for validation
)

test_datagen = ImageDataGenerator(rescale=1./255)

# Load Images as Batches
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),  # Adjust size based on model
    batch_size=32,
    class_mode='binary'  # Use 'categorical' if more than 2 classes
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

print("‚úÖ Data pipeline ready! üöÄ")

import os
print(os.path.exists(csv_path))  # Should return True

import os
import pandas as pd

# Define the correct path to the CSV file
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"  # Update if needed

# Check if the file exists
print(os.path.exists(csv_path))  # Should return True

# Load the CSV file (only if it exists)
if os.path.exists(csv_path):
    df = pd.read_csv(csv_path)
    print(df.head())  # Display first few rows
else:
    print("File not found! Check the path.")

import pandas as pd

# Load the dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"  # Update if needed
df = pd.read_csv(csv_path)

# Check if it loaded correctly
print(df.head())

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/AROGYA/TB_Chest_Xray/

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"
df = pd.read_csv(file_path)

# Rename 'sex' to 'gender' for consistency
df.rename(columns={'sex': 'gender'}, inplace=True)

# Drop missing values in necessary columns
df = df.dropna(subset=['findings', 'age', 'gender'])

# Simplify 'findings' column: Group all tuberculosis cases under 'tuberculosis'
df['findings'] = df['findings'].apply(lambda x: 'tuberculosis' if 'TB' in x or 'PTB' in x or 'ptb' in x else x)

# ‚úÖ 1Ô∏è‚É£ Class Distribution: Normal vs. TB
plt.figure(figsize=(8, 5))
sns.countplot(x='findings', data=df, order=['normal', 'tuberculosis'], palette={'normal': 'blue', 'tuberculosis': 'red'})
plt.xlabel("Findings")
plt.ylabel("Count")
plt.title("Class Distribution: Normal vs. TB")
plt.show()

# ‚úÖ 2Ô∏è‚É£ Age Distribution of Patients
plt.figure(figsize=(8, 5))
sns.histplot(df['age'], bins=30, kde=True, color="purple")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.title("Age Distribution of Patients")
plt.show()

# ‚úÖ 3Ô∏è‚É£ TB Cases by Gender
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="gender", hue="findings", palette="tab10", order=['Male', 'Female'])
plt.xlabel("Gender")
plt.ylabel("Count")
plt.title("TB Cases by Gender")
plt.legend(title="Findings", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

import os
print("Metadata File Exists:", os.path.exists("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"))

import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"
df = pd.read_csv(file_path)

# Standard Processing
df.columns = df.columns.str.lower()
if 'sex' in df.columns:
    df.rename(columns={'sex': 'gender'}, inplace=True)
df.dropna(subset=['findings', 'age', 'gender'], inplace=True)
df['findings'] = df['findings'].apply(lambda x: 'tuberculosis' if 'tb' in x.lower() else 'normal')

# ‚úÖ Save the Processed File
processed_path = "/content/shenzhen_processed.csv"
df.to_csv(processed_path, index=False)

print(f"‚úÖ Processed file saved at: {processed_path}")

import shutil
import os

# Define source and destination paths
source_path = "/content/shenzhen_processed.csv"
destination_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"

# Check if source file exists
if os.path.exists(source_path):
    # Move the file
    shutil.move(source_path, destination_path)
    print(f"‚úÖ File moved successfully to: {destination_path}")
else:
    print("‚ùå Source file does not exist!")

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_metadata.csv"  # Ensure correct path

# Check if file exists before loading
if not os.path.exists(file_path):
    raise FileNotFoundError(f"‚ùå Error: File not found at {file_path}")

print("‚úÖ Metadata file found. Loading dataset...")

df = pd.read_csv(file_path)

# Convert column names to lowercase for consistency
df.columns = df.columns.str.lower()

# Rename 'sex' column to 'gender' for compatibility with Arogya
if 'sex' in df.columns:
    df.rename(columns={'sex': 'gender'}, inplace=True)

# Check for missing values and drop them if found
df.dropna(subset=['findings', 'age', 'gender'], inplace=True)

# Standardize 'findings' values (Group all TB-related labels under 'tuberculosis')
df['findings'] = df['findings'].apply(lambda x: 'tuberculosis' if 'tb' in x.lower() else 'normal')

print("‚úÖ Data preprocessing complete.")

# Create the output directory if it doesn't exist
output_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray"
os.makedirs(output_dir, exist_ok=True)

# Visualizing Class Distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='findings', data=df, order=['normal', 'tuberculosis'], hue='findings',
              palette={'normal': 'blue', 'tuberculosis': 'red'}, legend=False)
plt.xlabel("Findings")
plt.ylabel("Count")
plt.title("Class Distribution: Normal vs. TB")
plt.show()

# Visualizing Age Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df['age'], bins=30, kde=True, color="purple")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.title("Age Distribution of Patients")
plt.show()

# Visualizing TB Cases by Gender
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x="gender", hue="findings", palette="tab10")
plt.xlabel("Gender")
plt.ylabel("Count")
plt.title("TB Cases by Gender")
plt.legend(title="Findings", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.show()

# Save processed data for Arogya integration
output_file = os.path.join(output_dir, "shenzhen_processed.csv")
df.to_csv(output_file, index=False)

print(f"‚úÖ Processed file saved at: {output_file}")

import os
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
print("File Exists:", os.path.exists(file_path))

import os

all_files = []
for root, dirs, files in os.walk("/content/drive/MyDrive/AROGYA/"):
    all_files.extend(files)

print(all_files)

import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(file_path)

# Display basic info
df.info()
df.head()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['age']] = scaler.fit_transform(df[['age']])

print(df.head())

processed_file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
df.to_csv(processed_file_path, index=False)

print("Processed dataset saved successfully!")

df.rename(columns=lambda x: x.strip(), inplace=True)  # Removes unwanted spaces

import pandas as pd

# Load Data
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"

try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print("Error: File not found. Ensure the file exists in the directory.")
    df = pd.DataFrame()  # Create an empty DataFrame as a fallback

# Ensure DataFrame is not empty
if df.empty:
    print("Warning: DataFrame is empty. Check data source.")

# Rename 'gender' to 'sex' (if it exists)
if 'gender' in df.columns:
    df.rename(columns={'gender': 'sex'}, inplace=True)

# Check unique values in 'sex' before processing
if 'sex' in df.columns:
    print("Unique values in 'sex' before processing:", df['sex'].unique())

    # Handle missing values in 'sex'
    if not df['sex'].mode().empty:
        mode_sex = df['sex'].mode()[0]  # Get the mode
        df['sex'] = df['sex'].fillna(mode_sex)
    else:
        df['sex'] = df['sex'].fillna("Unknown")

    # ‚úÖ Fix Mapping Issue: Convert 'Male' ‚Üí 0, 'Female' ‚Üí 1
    df['sex'] = df['sex'].map({'Male': 0, 'Female': 1})

# Handle missing values in 'age' (if column exists)
if 'age' in df.columns:
    df['age'] = df['age'].fillna(df['age'].median())

# Handle missing values in 'findings' (if column exists)
if 'findings' in df.columns:
    if not df['findings'].mode().empty:
        df['findings'] = df['findings'].fillna(df['findings'].mode()[0])
    else:
        df['findings'] = df['findings'].fillna("Unknown")

# Display first few rows to verify changes
print(df.head())

# Check for missing values after processing
print("Missing values after processing:")
print(df.isnull().sum())

# Print final column names
print("Final column names:", df.columns)

# Save processed file
final_file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
df.to_csv(final_file_path, index=False)

print("‚úÖ Processing complete! File saved as 'shenzhen_final_processed.csv'.")

import os
import cv2
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Paths
csv_file = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/"
processed_image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/processed_images/"

# Load dataset
df = pd.read_csv(csv_file)

# Ensure output folder exists
if not os.path.exists(processed_image_folder):
    os.makedirs(processed_image_folder)

# Image Processing Configuration
IMG_SIZE = (224, 224)  # ResNet50 standard input size

# Check for missing images
missing_images = []
for study_id in df["study_id"]:
    image_path = os.path.join(image_folder, study_id)
    if not os.path.exists(image_path):
        missing_images.append(study_id)
        continue

    # Read and preprocess image
    img = cv2.imread(image_path)
    img = cv2.resize(img, IMG_SIZE)  # Resize to 224x224
    img = img / 255.0  # Normalize pixel values

    # Save processed image
    processed_path = os.path.join(processed_image_folder, study_id)
    cv2.imwrite(processed_path, (img * 255).astype(np.uint8))  # Convert back to 0-255 for saving

# Print missing images
if missing_images:
    print(f"‚ö†Ô∏è Missing {len(missing_images)} images:")
    print(missing_images)
else:
    print("‚úÖ All images are present and processed.")

# Data Augmentation (If Needed)
augment = False  # Change to True if you want augmentation

if augment:
    datagen = ImageDataGenerator(
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True
    )

    for study_id in df["study_id"]:
        img_path = os.path.join(processed_image_folder, study_id)
        img = cv2.imread(img_path)
        img = np.expand_dims(img, axis=0)  # Convert to batch format

        # Generate augmented images
        aug_iter = datagen.flow(img, batch_size=1)
        aug_img = next(aug_iter)[0].astype(np.uint8)

        # Save augmented image
        aug_path = os.path.join(processed_image_folder, f"aug_{study_id}")
        cv2.imwrite(aug_path, aug_img)

    print("‚úÖ Data augmentation completed.")

import os
import shutil

# Define paths
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Ensure class folders exist
for category in ["Normal", "TB"]:
    os.makedirs(os.path.join(train_dir, category), exist_ok=True)
    os.makedirs(os.path.join(test_dir, category), exist_ok=True)

# Function to move images based on their labels
def move_images(source_dir):
    for filename in os.listdir(source_dir):
        src_path = os.path.join(source_dir, filename)

        if os.path.isdir(src_path):  # Skip directories
            continue

        # Determine the category based on filename
        if filename.endswith("_0.png"):
            dest_path = os.path.join(source_dir, "Normal", filename)
        elif filename.endswith("_1.png"):
            dest_path = os.path.join(source_dir, "TB", filename)
        else:
            print(f"‚ö†Ô∏è Unknown category: {filename} (Not moved)")
            continue

        # Move the file
        shutil.move(src_path, dest_path)

# Apply the function to both train and test directories
move_images(train_dir)
move_images(test_dir)

print("‚úÖ Images have been successfully moved to their categories!")

import os
import shutil

# Define paths
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Create class subfolders if they don‚Äôt exist
for category in ["Normal", "TB"]:
    os.makedirs(os.path.join(train_dir, category), exist_ok=True)
    os.makedirs(os.path.join(test_dir, category), exist_ok=True)

# Function to move images based on filename
def organize_images(directory):
    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)

        # Skip if it's already in a folder
        if os.path.isdir(file_path):
            continue

        # Move images based on category
        if "normal" in filename.lower():
            shutil.move(file_path, os.path.join(directory, "Normal", filename))
        elif "tb" in filename.lower() or "tuberculosis" in filename.lower():
            shutil.move(file_path, os.path.join(directory, "TB", filename))
        else:
            print(f"‚ö†Ô∏è Unknown category: {filename} (Not moved)")

# Move images in train and test sets
print("üìÇ Organizing train images...")
organize_images(train_dir)

print("üìÇ Organizing test images...")
organize_images(test_dir)

print("‚úÖ Images moved successfully!")

import os

train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
test_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

for category in ["Normal", "TB"]:
    train_path = os.path.join(train_dir, category)
    test_path = os.path.join(test_dir, category)

    print(f"üìÇ Checking: {train_path}")
    print(f"üñºÔ∏è Train Images: {len(os.listdir(train_path)) if os.path.exists(train_path) else 'Folder Not Found'}")

    print(f"üìÇ Checking: {test_path}")
    print(f"üñºÔ∏è Test Images: {len(os.listdir(test_path)) if os.path.exists(test_path) else 'Folder Not Found'}\n")

import pandas as pd
import os
import cv2
import numpy as np
import tensorflow as tf

# Load dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Step 1: Check if 'gender' column exists before renaming
if "gender" in df.columns:
    df.rename(columns={"gender": "sex"}, inplace=True)

# Step 2: Handling Missing Values
df.loc[:, 'sex'] = df.get('sex', pd.Series(['Unknown'] * len(df)))  # Fill missing column
df.loc[:, 'age'] = df.get('age', pd.Series([df['age'].median()] * len(df)))  # Fill missing ages

# Convert 'sex' column to numerical values
df['sex'] = df['sex'].map({'Male': 0, 'Female': 1}).fillna(-1)  # Assign -1 for Unknown

# Step 3: Image Processing
image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/"
processed_images = []

for study_id in df['study_id']:
    # Ensure study_id doesn't have extra .png
    study_id = str(study_id)  # Convert to string in case of NaN
    if not study_id.endswith(".png"):
        study_id += ".png"

    img_path = os.path.join(image_folder, study_id)

    if os.path.exists(img_path):  # Ensure file exists
        img = cv2.imread(img_path, cv2.IMREAD_COLOR)  # Load in color (for ResNet)
        img = cv2.resize(img, (224, 224))  # Resize for ResNet50

        # Apply proper ResNet preprocessing
        img = tf.keras.applications.resnet50.preprocess_input(img)

        processed_images.append(img)
    else:
        print(f"‚ö†Ô∏è Image not found: {img_path}")

# Convert list to NumPy array and save
if processed_images:
    processed_images = np.array(processed_images)
    np.save("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen/processed_images.npy", processed_images)
    print(f"‚úÖ Processed {len(processed_images)} images successfully!")
else:
    print("‚ùå No valid images processed. Check your dataset.")

import os

# Check if the directory exists
if os.path.exists(image_folder):
    print("‚úÖ Image folder exists.")
    print("üìÇ Sample files:", os.listdir(image_folder)[:10])  # List first 10 files
else:
    print("‚ùå Image folder not found! Check the path.")

import pandas as pd

# Define CSV path
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"

# Load dataset
df = pd.read_csv(csv_path)  # ‚úÖ Now df is defined

import pandas as pd

# Load dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Print column names to check
print("üîç Column names in dataset:", df.columns.tolist())  # Debugging step

df.rename(columns={"gender": "sex"}, inplace=True)

if df.empty:
    raise ValueError("‚ùå CSV file is empty or not loading correctly!")

print(df.head())  # Show first few rows to verify data

# Create a new copy to avoid chained assignment warning
df = df.copy()

# Fill missing values
df['sex'] = df['sex'].fillna('Unknown')
df['age'] = df['age'].fillna(df['age'].median())

import os
import pandas as pd

# Load dataset
csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Check if 'study_id' exists in DataFrame
if 'study_id' not in df.columns:
    raise ValueError("‚ùå Column 'study_id' not found in dataset. Check your CSV file!")

# Define the image folder path
image_folder = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/"

# Check image paths
for study_id in df['study_id']:
    # Ensure we don't add ".png" again if it's already there
    if not study_id.endswith(".png"):
        study_id += ".png"

    img_path = os.path.join(image_folder, study_id)

    if not os.path.exists(img_path):
        print(f"‚ö†Ô∏è Image not found: {img_path}")

"""Train the Model (Use GPU for Speed ‚ö°)


‚úÖ Start with ResNet50 (Pretrained on ImageNet).

‚úÖ Use Google Colab Pro or Kaggle GPUs for faster training.


‚úÖ Save model weights after each epoch to avoid retraining from scratch.

"""

y = df['findings'].values  # Use 'findings' as the label

print(df['findings'].unique())  # Check what labels are present

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y = encoder.fit_transform(df['findings'])  # 'normal' ‚Üí 0, 'tuberculosis' ‚Üí 1

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")

"""TRAINING
=========================================================================

"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Load Pretrained ResNet50 (without top layers)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers
x = Flatten()(base_model.output)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output = Dense(1, activation='sigmoid')(x)  # Binary classification (Normal vs TB)

# Compile model
model = Model(inputs=base_model.input, outputs=output)
model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.summary()  # Check model architecture

history = model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))

from tensorflow.keras.applications import VGG16

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# ‡§Ö‡§¨ ‡§Ü‡§ñ‡§ø‡§∞‡•Ä ‡§ï‡•á 20 ‡§≤‡•á‡§Ø‡§∞‡•ç‡§∏ ‡§ï‡•ã trainable ‡§¨‡§®‡§æ‡§®‡§æ
for layer in base_model.layers[-20:]:
    layer.trainable = True

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    rescale=1./255
)

from tensorflow.keras.layers import Flatten, Dense, Dropout

x = Flatten()(base_model.output)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(1, activation='sigmoid')(x)

from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

"""doneeeeeeeeee"""

import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"  # Ensure this is correct
df = pd.read_csv(file_path)

# Check if the dataset is loaded properly
print(df.head())  # Display first few rows
print(df.columns)  # Display column names

import os
print(os.listdir("/content/drive/MyDrive/AROGYA/TB_Chest_Xray"))

df = pd.read_csv(file_path, encoding='latin1')

import pandas as pd

# Correct file path
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"

# Load dataset with encoding handling
try:
    df = pd.read_csv(file_path, encoding='utf-8')  # Try utf-8 encoding first
    print(df.head())  # Display first few rows
    print(df.columns)  # Display column names
except UnicodeDecodeError:
    df = pd.read_csv(file_path, encoding='ISO-8859-1')  # Fallback encoding
    print(df.head())
    print(df.columns)

print(df["findings"].value_counts())

X_train = df[['sex', 'age']].values  # Features
y_train = df['findings'].values  # Labels

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_train = encoder.fit_transform(y_train)  # Convert labels to numbers

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(class_weights))
print(class_weights)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input

# Load dataset
file_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_final_processed.csv"
df = pd.read_csv(file_path, encoding='utf-8')

# Display dataset summary
print(df.head())
print(df["findings"].value_counts())

# Feature & label extraction
X = df[['sex', 'age']].values
y = df['findings'].values

# Label encoding
encoder = LabelEncoder()
y = encoder.fit_transform(y)  # Convert labels to numbers (0=Normal, 1=TB)

# üîπ Normalize Age (Feature Scaling)
scaler = MinMaxScaler()
X[:, 1] = scaler.fit_transform(X[:, 1].reshape(-1, 1)).flatten()

# Split dataset into Train & Validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Compute class weights (for imbalanced dataset)
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights = dict(enumerate(class_weights))
print(class_weights)

# ‚úÖ Improved Model Architecture
model = Sequential([
    Input(shape=(X_train.shape[1],)),  # Fix UserWarning
    Dense(32, activation='relu'),
    Dropout(0.2),  # Regularization
    Dense(16, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # Binary classification
])

# ‚úÖ Compile with lower learning rate
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# ‚úÖ Train the Model
history = model.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=20,  # More epochs
    batch_size=32,
    class_weight=class_weights
)

"""NOW WORKIG TO IMPROVE THE EFFICIENCY AND FURTHER

"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rescale=1./255,      # Normalize pixels
    rotation_range=10,   # Rotate images slightly
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True
)

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model

base_model = ResNet50(weights="imagenet", include_top=False, input_shape=(224, 224, 3))
x = Flatten()(base_model.output)
x = Dense(256, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1, activation="sigmoid")(x)  # Binary classification

model = Model(inputs=base_model.input, outputs=x)
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])



import os
print(os.listdir("/content"))

from google.colab import drive
drive.mount('/content/drive')

import os
print(os.listdir("/content/drive/MyDrive/AROGYA/TB_Chest_Xray"))

train_data = datagen.flow_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train",  # Use the correct subfolder
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary"
)

val_data = datagen.flow_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test",  # Use the correct subfolder
    target_size=(224, 224),
    batch_size=32,
    class_mode="binary"
)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=50,  # Increase if needed
    callbacks=[lr_scheduler]  # Ensure lr_scheduler is defined
)

"""TESTING AND SAVINGS THE THINGS DONE
==============================================================================

"""

import os

h5_model_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.h5"
print("File exists:", os.path.exists(h5_model_path))

import numpy as np
import tensorflow as tf

# Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")
interpreter.allocate_tensors()

# Get input and output details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Create a random test input (assuming input shape is 224x224x3)
test_input = np.random.rand(1, 224, 224, 3).astype(np.float32)

# Run inference
interpreter.set_tensor(input_details[0]['index'], test_input)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

print("üü¢ TFLite Model Output:", output_data)

import cv2

# Load and preprocess an actual X-ray image
image_path = "/content/drive/MyDrive/AROGYA/sample_xray.jpg"
image = cv2.imread(image_path)
image = cv2.resize(image, (224, 224))  # Resize to match model input size
image = image.astype(np.float32) / 255.0  # Normalize
image = np.expand_dims(image, axis=0)  # Add batch dimension

# Run inference
interpreter.set_tensor(input_details[0]['index'], image)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])

print("üü¢ TFLite Model Output:", output_data)

import tensorflow as tf

# Define paths
h5_model_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.h5"
tflite_model_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite"

# Load the existing .h5 model
model = tf.keras.models.load_model(h5_model_path)

# Convert to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the .tflite model
with open(tflite_model_path, "wb") as f:
    f.write(tflite_model)

print("‚úÖ Model successfully converted and saved at:", tflite_model_path)

"""
TO MOUNT DRIVE EXECUTE THE FOLLOWING CODES
============================================================================="""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

import pandas as pd

csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Print the first few rows
print(df.head())

# Print all column names
print(df.columns)

df.columns = df.columns.str.strip()  # Removes leading/trailing spaces

import pandas as pd

csv_path = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv"
df = pd.read_csv(csv_path)

# Print all column names
print("Columns in CSV:", df.columns.tolist())

print(df.head())  # Shows first few rows

import os

# Define the directory where images are stored
image_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/"

# Generate image paths (assuming filenames match study_id)
df["image_path"] = df["study_id"].astype(str) + ".png"  # Adjust extension if needed
df["image_path"] = df["image_path"].apply(lambda x: os.path.join(image_dir, x))

# Save the updated CSV with image paths
df.to_csv("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/shenzhen_processed.csv", index=False)

print("Image paths added successfully! ‚úÖ")

image_paths = df['image_path'].values
labels = df['findings'].apply(lambda x: 1 if x.lower() == 'tuberculosis' else 0).values

import os

# Fix duplicate extensions
image_paths = [os.path.join("/content/drive/MyDrive/AROGYA/TB_Chest_Xray/images/images/", path.replace(".png.png", ".png")) for path in df['study_id']]

# Verify the first corrected path
print(image_paths[0])
print(os.path.exists(image_paths[0]))  # Should print True now

from PIL import Image
img = Image.open(image_paths[0])
img.show()  # Display the image

from PIL import Image

# Open and display the image
img = Image.open(image_paths[0])
img.show()  # Should display the image

import os
import cv2
import matplotlib.pyplot as plt

image_path = image_paths[0]  # First image path

if os.path.exists(image_path):
    print(f"File exists: {image_path}")
    img = cv2.imread(image_path, cv2.IMREAD_COLOR)

    if img is not None:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
        plt.imshow(img)
        plt.title("X-ray Image")
        plt.show()
    else:
        print("‚ö† OpenCV failed to load the image. Check file format or permissions.")
else:
    print("üö® Image file does not exist. Verify the file path!")

import cv2
import matplotlib.pyplot as plt

# Load the image
img = cv2.imread(image_path)

# Check if the image was loaded successfully
if img is None:
    print("üö® Error: Failed to load the image. Check the file format and path!")
else:
    # Convert BGR to RGB (since OpenCV loads images in BGR format)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Display the image
    plt.imshow(img)
    plt.axis("off")  # Hide axes
    plt.title("Chest X-ray")
    plt.show()

"""Backend testing
==============================================================================
"""

import tensorflow as tf
print(tf.__version__)  # Check TensorFlow version

import tensorflow as tf

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")

# Allocate tensors
interpreter.allocate_tensors()

# Get input details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Print input & output tensor details
print("Input details:", input_details)
print("Output details:", output_details)

import cv2
import numpy as np
import tensorflow as tf

# Load TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")
interpreter.allocate_tensors()

# Get input & output tensor indices
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Load and preprocess an actual X-ray image
image_path = "/content/drive/MyDrive/AROGYA/sample_xray.jpg"
image = cv2.imread(image_path)

# Verify if the image was loaded correctly
if image is None:
    raise FileNotFoundError(f"‚ùå Image not found at: {image_path}")

# Resize & normalize image
image = cv2.resize(image, (224, 224))
image = image.astype(np.float32) / 255.0  # Normalize to [0,1]
image = np.expand_dims(image, axis=0)  # Add batch dimension

# Set input tensor
interpreter.set_tensor(input_details[0]['index'], image)

# Run inference
interpreter.invoke()

# Get output tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

# Print the result
print("üü¢ Model Output:", output_data)

# Interpret output (assuming binary classification: [0] = Normal, [1] = TB)
prob_TB = output_data[0][1]  # Probability of TB
prob_Normal = output_data[0][0]  # Probability of Normal

if prob_TB > prob_Normal:
    print(f"üî¥ Prediction: Tuberculosis detected with {prob_TB:.2%} confidence")
else:
    print(f"üü¢ Prediction: Normal with {prob_Normal:.2%} confidence")

import cv2
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from google.colab import files
from PIL import Image

# 1Ô∏è‚É£ Ask user to upload an X-ray image
uploaded = files.upload()  # Opens file upload prompt

# Get the uploaded file name
image_name = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded file: {image_name}")

# Load the uploaded image
image = Image.open(image_name).convert("L")  # Convert to grayscale

# Display the uploaded image
plt.imshow(image, cmap='gray')
plt.title("Uploaded X-ray Image")
plt.axis("off")
plt.show()

# Convert image to OpenCV format
image = np.array(image)

# 2Ô∏è‚É£ Preprocess the image for the model
image = cv2.resize(image, (224, 224))  # Resize to model input size
image = np.expand_dims(image, axis=-1)  # Add channel dimension (grayscale)
image = np.repeat(image, 3, axis=-1)  # Convert grayscale to 3-channel RGB
image = image.astype(np.float32) / 255.0  # Normalize to [0,1]
image = np.expand_dims(image, axis=0)  # Add batch dimension

# 3Ô∏è‚É£ Load the TFLite model
interpreter = tf.lite.Interpreter(model_path="/content/drive/MyDrive/AROGYA/TB_Chest_Xray/TB_Model.tflite")
interpreter.allocate_tensors()

# Get input & output tensor details
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Set the input tensor
interpreter.set_tensor(input_details[0]['index'], image)

# Run inference
interpreter.invoke()

# Get the output tensor
output_data = interpreter.get_tensor(output_details[0]['index'])

# Print the result
print("üü¢ Model Output:", output_data)

# 4Ô∏è‚É£ Interpret output (assuming binary classification: [0] = Normal, [1] = TB)
prob_TB = output_data[0][1]  # Probability of TB
prob_Normal = output_data[0][0]  # Probability of Normal

if prob_TB > prob_Normal:
    print(f"üî¥ Prediction: Tuberculosis detected with {prob_TB:.2%} confidence")
else:
    print(f"üü¢ Prediction: Normal with {prob_Normal:.2%} confidence")

pip install --upgrade tensorflow

"""


Flask API DEPENDENCIES SET UP TRY
====================================
"""

from google.colab import drive
drive.mount('/content/drive')

import os
print(os.listdir("/content/drive/MyDrive/AROGYA/TB_Chest_Xray"))

import tensorflow as tf

train_data = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train",  # Updated path
    image_size=(224, 224),
    batch_size=32
)

val_data = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test",  # Updated path
    image_size=(224, 224),
    batch_size=32
)

import matplotlib.pyplot as plt
import numpy as np

class_names = train_data.class_names  # Get class labels

plt.figure(figsize=(10, 10))
for images, labels in train_data.take(1):  # Take one batch
    for i in range(9):  # Show 9 images
        plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(class_names[labels[i]])
        plt.axis("off")
plt.show()

print("Train dataset shape:", train_data)
print("Validation dataset shape:", val_data)
print("Class labels:", train_data.class_names)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Define CNN model
model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),  # Reduce overfitting
    layers.Dense(2, activation='softmax')  # 2 classes: Normal, TB
])

# Compile model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Summary of the model
model.summary()

# Load dataset using ImageDataGenerator
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define directories
train_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/train"
val_dir = "/content/drive/MyDrive/AROGYA/TB_Chest_Xray/test"

# Data Augmentation & Preprocessing
train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)

# Load the training and validation datasets
train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse'  # Matches sparse_categorical_crossentropy
)

val_data = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse'
)

# Now train the model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    verbose=1
)

"""FRONTEND
====================

"""

from google.colab import drive
drive.mount('/content/drive')

!pip install flask-ngrok

!pip install flask

!pip install flask flask-ngrok pyngrok numpy opencv-python tensorflow

import ipywidgets as widgets
from IPython.display import display
import requests

# Create a file upload widget
upload = widgets.FileUpload(accept='.jpg,.png,.jpeg', multiple=False)
display(upload)

def upload_and_predict(change):
    if upload.value:
        file_info = next(iter(upload.value.values()))
        file_name = file_info['metadata']['name']

        # Save uploaded file
        with open(file_name, 'wb') as f:
            f.write(file_info['content'])

        # API endpoint
        url = "http://xyz.ngrok.io/predict"

        # Upload the chosen image
        try:
            with open(file_name, 'rb') as f:
                files = {'file': f}
                response = requests.post(url, files=files)

            # Check if the response is valid JSON
            try:
                result = response.json()
                print("Prediction Result:", result)
            except requests.exceptions.JSONDecodeError:
                print("Error: Response is not valid JSON. Response text:", response.text)

        except requests.exceptions.RequestException as e:
            print("Request failed:", e)

upload.observe(upload_and_predict, names='value')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define ImageDataGenerator for loading dataset
train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_data = train_datagen.flow_from_directory(
    'dataset_directory',  # Replace with actual dataset path
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='training'
)

val_data = train_datagen.flow_from_directory(
    'dataset_directory',
    target_size=(224, 224),
    batch_size=32,
    class_mode='sparse',
    subset='validation'
)

# Define the model
model = keras.Sequential([
    layers.Input(shape=(224, 224, 3)),  # Corrected input shape
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(2, activation='softmax')  # 2 classes (Normal, TB)
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    verbose=1
)